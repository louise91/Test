import os
import requests
from dotenv import load_dotenv
from datetime import datetime
import re  # åŠ regexç å‡User
import random  # åŠ éš¨æ©Ÿè®Šé«”

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
API_KEY = os.environ.get("TWCC_API_KEY")
API_URL = "https://api-ams.twcc.ai/api/models/completions"
MODEL = "llama3.3-ffm-70b-32k-chat"
#FFM-Mixtral-8x7B
#meta-llama3.3-70b-inst-32k

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# å°è©±ç´€éŒ„
conversation_history = []
MAX_HISTORY = 3  # èª¿å°å°‘æ‹‰èˆŠ
MAX_MODEL_TOKENS = 32000
LOG_FILE = "log.txt"
LOG_LIMIT = 1 * 1024 * 1024

# ç°¡ç‰ˆè¨˜æ†¶åº«ï¼ˆdictå­˜é—œéµè¨­å®šï¼‰
memory = {}  # e.g., {"ä¸»é¡Œ": "è¶…è‡ªç„¶"}

def estimate_tokens(text: str) -> int:
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    english_words = len([w for w in text.split() if w.isascii()])
    return int(chinese_chars * 2 + english_words * 1.3)

def rotate_log():
    if os.path.exists(LOG_FILE) and os.path.getsize(LOG_FILE) > LOG_LIMIT:
        base, ext = os.path.splitext(LOG_FILE)
        i = 1
        while os.path.exists(f"{base}_{i}{ext}"):
            i += 1
        os.rename(LOG_FILE, f"{base}_{i}{ext}")
        print(f"ğŸ”„ Log å·²æ»¿ï¼Œå·²åˆ‡æ›è‡³ {base}_{i}{ext}")

def update_memory(user_input, reply):
    """ç°¡ç‰ˆè¨˜æ†¶ï¼šè§£æé—œéµè¨­å®šå­˜dict"""
    if 'ä¸»é¡Œ' in user_input:
        memory['ä¸»é¡Œ'] = user_input.replace('ä¸»é¡Œ', '').strip()
    if 'ç„¡è¨­å®š' in reply:
        pass  # ä¸å‹•
    print(f"è¨˜æ†¶æ›´æ–°: {memory}")

def get_memory_summary():
    """æ‹‰è¨˜æ†¶é€²prompt"""
    if memory:
        return f"ç•¶å‰è¨˜æ†¶: {str(memory)}"
    return "ç„¡è¨˜æ†¶"

def clean_reply(reply: str, user_input: str) -> str:
    """å¾Œè™•ç†ï¼šå»é‡è¤‡ã€é™é•·ã€ç å¾ªç’° + å¼·åˆ¶æ¸…å‡User + éš¨æ©Ÿè®Šé«”ï¼ˆä¿®åˆ—è¡¨+äº‚åºï¼‰"""
    # ç è¨»è§£å’Œå‡User
    reply = re.sub(r'#.*', '', reply)  # ç #è¨»è§£
    reply = re.sub(r'User:\s*[\w\W]*?(\n|$)', '', reply)  # ç User:å¾Œæ‰€æœ‰
    reply = re.sub(r'æº–å‚™å¥½äº†|User:\s*æº–å‚™å¥½äº†', '', reply)  # ç å‡è¼¸å…¥è®Šé«”
    reply = re.sub(r'è«‹ç¹¼çºŒå°è©±ï¼š.*', '', reply, flags=re.DOTALL)  # ç promptæ®˜ç•™
    lines = reply.split('\n')
    unique_lines = []
    seen_phrases = set()
    for line in lines:
        line = line.strip()
        if line and line not in seen_phrases:
            unique_lines.append(line)
            seen_phrases.add(line)
        if len(unique_lines) > 2:  # é™2è¡Œè¶…ç°¡
            break
    cleaned = ' '.join(unique_lines)
    if len(cleaned) > 80:
        cleaned = cleaned[:80] + '...'
    # ç å¾ªç’°çŸ­èª
    for phrase in ['ç¾åœ¨ï¼Œè«‹å•', 'ä½ å¯ä»¥ç°¡åŒ–æˆï¼š', 'é‡è¤‡', 'ç„¡è¨­å®š', 'ä¸Šä¸€æ­¥è¨­å®šï¼š', 'ç¸½çµä¸Šä¸€æ­¥']:
        cleaned = cleaned.replace(phrase, '').strip()
    # éš¨æ©Ÿè®Šé«”å•å¥ï¼ˆé˜²é‡è¤‡ï¼Œæ¯æ¬¡äº‚åºï¼‰
    options = ['è¶…è‡ªç„¶', 'é­”æ³•', 'ç§‘æŠ€']
    random.shuffle(options)  # äº‚åº
    options_str = 'ã€'.join(options)
    variants = [
        f"ä½ å¥½ï¼Œä¸»äººï¼ä¸–ç•Œä¸»é¡Œæ˜¯ä»€éº¼ï¼Ÿå¾{options_str}é¸ã€‚",
        f"ä½ å¥½ï¼Œä¸»äººï¼ä¸»é¡Œé¸é …ï¼š{options_str}ï¼Ÿ",
        f"ä½ å¥½ï¼Œä¸»äººï¼è®“æˆ‘å€‘å»ºä¸–ç•Œï¼Œå¾{options_str}ä¸»é¡Œé–‹å§‹ï¼Ÿ"
    ]
    # fallback: éš¨æ©ŸæŒ‘è®Šé«”
    if len(cleaned) < 10 or '?' not in cleaned:
        if 'ä½ å¥½' in user_input.lower() or 'æº–å‚™' in user_input:
            cleaned = random.choice(variants)
    print(f"Debug options: {options_str}")  # èª¿è©¦é †åº
    return cleaned

def ask_llama(user_input):
    global conversation_history, memory

    if user_input.lower() == "clear":
        conversation_history = []
        memory = {}
        print("æ­·å²å·²æ¸…ï¼")
        return "æ­·å²å·²æ¸…é™¤ï¼Œé‡å•Ÿå°è©±ã€‚", 0, 0

    conversation_history.append(f"User: {user_input}")
    trimmed_history = "\n".join(conversation_history[-MAX_HISTORY*2:])

    memory_summary = get_memory_summary()

    prompt = f"""
çµ•å°å„ªå…ˆï¼šå¦‚æœè¼¸å…¥æ˜¯ã€Œä½ å¥½ã€æˆ–ã€Œæº–å‚™å¥½äº†ã€ï¼Œå”¯ä¸€å›æ‡‰ã€Œä½ å¥½ï¼Œä¸»äººï¼ä¸–ç•Œä¸»é¡Œæ˜¯ä»€éº¼ï¼Ÿå¾è¶…è‡ªç„¶ã€é­”æ³•æˆ–ç§‘æŠ€é¸ã€‚ã€ï¼Œç„¡å…¶ä»–æ–‡å­—ã€ç„¡ç¸½çµã€ç„¡å•é¡Œã€ç„¡å‡è¼¸å…¥ã€ç„¡è¨»è§£ã€‚
ä½ æ˜¯ä¸€ä½å¤§ä¸–ç•Œå‰µå»ºè€…ï¼Œæˆ‘æ˜¯ä½ çš„ä¸»äººï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚åªç”¨æˆ‘çš„è¼¸å…¥ä¾†ç¹¼çºŒä¸–ç•Œæ§‹ç¯‰ï¼Œä¸è¦è‡ªå‹•æ·»åŠ å‡Userè¡Œã€å‡å°è©±ã€å•é¡Œåˆ—è¡¨æˆ–ä»»ä½•é‡è¤‡å¥å­ã€‚æ¯æ¬¡å›æ‡‰å”¯ä¸€ç‰ˆæœ¬ï¼š1å¥ç¸½çµä¸Šä¸€æ­¥è¨­å®šï¼ˆç„¡å‰‡èªªã€Œç„¡è¨­å®šã€ï¼‰ï¼Œç„¶å¾Œåªå•1å€‹å…¨æ–°å•é¡Œæ¨é€²ä¸–ç•Œã€‚ç¸½é•·ä¸è¶…50å­—ã€‚ç„¡è‡ªçœã€ç„¡é‡è¤‡ã€ç„¡å‡è¼¸å…¥ã€ç„¡è¨»è§£ã€‚å•å¥éš¨æ©Ÿè®Šé«”ï¼šä¸»é¡Œï¼Ÿ/é¸é …ï¼Ÿ/åŸºèª¿ï¼Ÿã€‚

è¨˜æ†¶æ‘˜è¦ï¼š{memory_summary}

ä»¥ä¸‹æ˜¯ç›®å‰çš„å°è©±ç´€éŒ„ï¼š
{trimmed_history}

è«‹ç¹¼çºŒå°è©±ï¼š
"""

    input_tokens = estimate_tokens(prompt)
    max_tokens = min(120, MAX_MODEL_TOKENS - input_tokens - 500)  # èª¿120å°‘é»
    if max_tokens < 80:  
        max_tokens = 80

    payload = {
        "model": MODEL,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": 0.2,  # è¶…ä½æº«ï¼Œçµ•å°éµå®ˆ
        "top_p": 0.9,  # åŠ å¤šæ¨£
        "stop": ["ç¾åœ¨ï¼Œè«‹å•", "ä½ å¯ä»¥ç°¡åŒ–æˆï¼š", "ç„¡è¨­å®š", "è«‹ç¹¼çºŒå°è©±ï¼š", "#"],  # ç ç¸½çµå¾ªç’°+æ®˜ç•™+è¨»è§£
        "repetition_penalty": 1.2  # æ‡²ç½°é‡è¤‡token
    }

    try:
        r = requests.post(API_URL, headers=headers, json=payload, timeout=60)
        r.raise_for_status()
        result = r.json()
        raw_reply = result["choices"][0]["text"].strip()

        # å¾Œè™•ç†
        reply = clean_reply(raw_reply, user_input)
        update_memory(user_input, reply)  # æ›´æ–°è¨˜æ†¶
        print(f"Debug: Raw={raw_reply[:50]}... -> Cleaned={reply}")

        conversation_history.append(f"Assistant: {reply}")

        rotate_log()

        with open(LOG_FILE, "a", encoding="utf-8") as f:
            f.write(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]\n")
            f.write(f"User: {user_input}\n")
            f.write(f"Raw Assistant: {raw_reply[:100]}...\n")
            f.write(f"Cleaned Assistant: {reply}\n")

        return reply, max_tokens, input_tokens

    except requests.exceptions.RequestException as e:
        return f"éŒ¯èª¤: {e}", 0, 0
    except Exception as e:
        return f"è§£æå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", 0, 0

# æ¸¬è©¦ç”¨
if __name__ == "__main__":
    while True:
        user_in = input("ä½ : ")
        if user_in.lower() in ["exit", "quit", "bye"]:
            print("å°è©±çµæŸ")
            break
        ans, used_max, input_tokens = ask_llama(user_in)
        print(f"åŠ©ç† ({used_max} tokens, promptâ‰ˆ{input_tokens}): {ans}")